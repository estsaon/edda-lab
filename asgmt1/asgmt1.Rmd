---
title: "Assignment 1"
author: "Peter Pan, Yuxiang Wang, Yancheng Zhuang"
date: "19 February 2021"
output: pdf_document
fontsize: 11pt
highlight: tango
header-includes: \usepackage{bbm}
---

`r options(digits=3)`

## Exercise 1 
<!--
The data set birthweight.txt contains the birthweights of 188 newborn babies. 
We are interested in finding the underlying (population) mean μ of birthweights.
-->

**a)** 
<!--
Check normality of the data. 
Compute a point estimate for μ. 
Derive, assuming normality (irrespective of your conclusion about normality od the data), a bounded 90% confidence interval for μ. 
-->
We check the normality of the data using QQ-plot. 

```{r, fig.height=3, fig.width=6, fig.align='center'}
birth_weight <- scan("birthweight.txt", skip=1)
qqnorm(birth_weight)
```

The points in the QQ-plot are approximately on a straight line, so we assume that the data are sampled from the normal distribution.
Because we don't know both the $\mu$ and $\delta$, we derive the 90% confidence interval for $\mu$ using $t$-confidence interval.

```{r}
res <- t.test(birth_weight, conf.level=0.9)
```

We compute $\bar{x}$ = `r res$estimate` as a point estimate for $\mu$ and its 90% confidence interval is [`r res$conf.int`]. 

**b)** 
<!--
An expert claims that the mean birthweight is bigger than 2800, verify this claim by using a t-test.
What is the outcome of the test if you take α = 0.1? 
And other values of α?
-->
According to what expert claims, we set up null hypothesis $H_0$: $\mu \leq 2800$. 

```{r}
res <- t.test(birth_weight, alternative=c("greater"), mu=2800, conf.level=0.9)
```

For $\alpha = 0.1$, CI = [`r res$conf.int`], $p$ = `r res$p.value`. 
Since $p$ < $\alpha$, we reject $H_0$ and support the claim. 
For other $\alpha$, only when $\alpha$ < $p$, i.e. `r res$p.value`, the claim will be rejected. 

**c)** 
<!--
In the R-output of the test from b), also a confidence interval is given, 
but why is it different from the confidence interval found in a) and why is it one-sided?
-->
For a), the confidence interval is constructed for the point estimation of $\mu$, 
but for b) the confidence interval is constructed for the one-sided hypothesis test on the claim. 
And also for the same reason the one-sided hypothesis resulted in the one-sided confidence interval. 

## Exercise 2 
<!--
We study the power function of the two-sample t-test (see Section 1.9 of Assignment 0). 
For n=m=30, mu=180, nu=175 and sd=5, generate 1000 samples x=rnorm(n, mu, sd) and y=rnorm(m, nu, sd), 
and record the 1000 p-values for testing H0: mu=nu. 
You can evaluate the power (at point nu=175) of this t-test as fraction of p-values that are smaller than 0.05.
-->

**a)** 
<!--
Set n=m=30, mu=180 and sd=5. 
Calculate now the power of the t-test for every value of nu in the grid seq(175, 185, by=0.25). 
Plot the power as a function of nu.
-->
```{r fig.height=3, fig.width=6, fig.align='center'}
plot_nu_power <- function(m, n, mu, nus, sd, B, col, add) {
    p_values <- numeric(B); power <- numeric(length(nus))
    for (i in 1:length(nus)) {
        for (b in 1:B) {
            x <- rnorm(m, mu, sd)
            y <- rnorm(n, nus[i], sd)
            p_values[b] <- t.test(x, y, var.equal=TRUE)$p.value
        }
        power[i] <- mean(p_values < 0.05)
    }
    if (add) {
        par(new=TRUE)
        plot(nus, power, type="p", col=col, axes=FALSE, ylab='', ylim=c(0, 1))
    } else {
        plot(nus, power, type="p", col=col, ylim=c(0, 1))
    }
}

plot_nu_power(30, 30, 180, seq(175, 185, by=0.25), 5, 1000, "red", FALSE)
```

**b)** 
<!--
Set n=m=100, mu=180 and sd=5. 
Repeat the preceding exercise. 
Add the plot to the preceding plot.
-->
```{r fig.height=3, fig.width=6, fig.align='center'}
plot_nu_power(30, 30, 180, seq(175, 185, by=0.25), 5, 1000, "red", FALSE)
plot_nu_power(100, 100, 180, seq(175, 185, by=0.25), 5, 1000, "blue", TRUE)
```

**c)** 
<!--
Set n=m=30, mu=180 and sd=15. 
Repeat the preceding exercise.
-->
```{r fig.height=3, fig.width=6, fig.align='center'}
plot_nu_power(30, 30, 180, seq(175, 185, by=0.25), 5, 1000, "red", FALSE)
plot_nu_power(100, 100, 180, seq(175, 185, by=0.25), 5, 1000, "blue", TRUE)
plot_nu_power(30, 30, 180, seq(175, 185, by=0.25), 15, 1000, "green", TRUE)
```


**d)** 
<!--
Explain your findings.
-->
The power of $t$-test gets smaller when nu gets closer to 180, this is because mu = 180 and thus it is less possible to reject the null hypothesis. 
If we let the size of samples bigger, then nu has to be closer to mu to get a lower probability of rejection, which is more accurate. 
If we enlarge the standard deviation, which indicates that the samples can spread far from the nu and mu, and thus let the test be more difficult to reject to null hypothesis, so the power gets lower.

## Exercise 3
<!--
A telecommunication company has entered the market for mobile phones in a new country. 
The company's marketing manager conducts a survey of 200 new subscribers for mobile phones. 
The results of the survey are in the data set telephone.txt, 
which contains the first month bills X_1, ..., X_200, in euros.
-->

**a)** 
<!--
Make an appropriate plot of this data set. 
What marketing advice(s) would you give to the marketing manager? 
Are there any inconsistencies in the data? 
If so, try to fix these.
-->

**b)** 
<!--
By using a bootstrap test with the test statistic T = median(X_1, ..., X_200), 
test whether the data telephone.txt stems from the exponential distribution Exp(λ) with some λ from [0.01, 0.1].
-->

**c)** 
<!--
Construct a 95% bootstrap confidence interval for the population median of the sample.
-->

**d)** 
<!--
Assuming X_1, ..., X_N ~ Exp(λ) and using the central limit theorem for the sample mean, 
estimate λ and construct again a 95% confidence interval for the population median. 
Comment on your findings.
-->

**e)** 
<!--
Using an appropriate test, test the null hypothesis that the median bill is bigger or equal to 40 euro against 
the alternative that the median bill is smaller than 40 euro. 
Next, design and perform a test to check whether the fraction of the bills less than 10 euro is less than 25%.
-->

## Exercise 4
<!--
To study the effect of energy drink a sample of 24 high school pupils were randomized to drinking 
either a softdrink or an energy drink after running for 60 meters. 
After half an hour they were asked to run again. 
For both sprints they were asked to sprint as fast they could, and the sprinting time was measured. 
The data is given in the file run.txt. 
[Courtesy class 5E, Stedelijk Gymnasium Leiden, 2010.]
-->

**a)** 
<!--
Disregarding the type of drink, test whether the run times before drink and after are correlated. 
-->

First we check the normality of the run times using QQ-plot. 

```{r fig.height=3, fig.width=6, fig.align='center'}
run <- read.table("run.txt", header=TRUE)
par(mfrow=c(1, 2))
qqnorm(run$before); qqnorm(run$after)
```

According to the QQ-plot, we think the data can be assumed normality. 
So we use Pearson's correlation test to test correlation. 
We set up $H_0$ as there is no correlation between the run times before drink and after. 

```{r}
p_val <- cor.test(run$before, run$after)$p.value
```

The results show that $p$ = `r p_val`, so we reject the null hypothesis. 
Therfore, the run times before drink and after are correlated.

**b)** 
<!--
Test separately, for both the softdrink and the energy drink conditions, 
whether there is a difference in speed in the two running tasks.
-->

We first check the nomality of the differences in speed using QQ-plot. 

```{r fig.height=3, fig.width=6, fig.align='center'}
par(mfrow=c(1, 2))
run_energy <- subset(run, drink=="energy")
run_lemo <- subset(run, drink=="lemo")
qqnorm(run_energy$before - run_energy$after)
qqnorm(run_lemo$before - run_lemo$after)
```

According to the plot, we cannot assume the normality.
So we use permutation test here with mean as test statistic.
We set up $H_0$ as there is no difference in speeds in the two running tasks. 

```{r}
perm.test <- function(x, y, T, B) {
   T_val <- T(x - y)
   T_vals <- numeric(B)

   for (i in 1:B) {
      perm_data <- t(apply(cbind(x, y), 1, sample))
      T_vals[i] <- T(perm_data[, 1] - perm_data[, 2])
   }

   p_val_left <- sum(T_vals < T_val) / B
   p_val_right <- sum(T_vals > T_val) / B
   p_val <- 2 * min(p_val_left, p_val_right)
   return (p_val)
}
p_val_energy <- perm.test(run_energy$before, run_energy$after, mean, 1000)
p_val_lemo <- perm.test(run_energy$before, run_energy$after, mean, 1000)
```

We get $p_{energy}$ = `r p_val_energy` and  $p_{lemo}$ = `r p_val_lemo`. 
So for both of the drink we don't reject null hypothesis. 
Therefore, there is no difference in speed in the two running tasks.

**c)** 
<!--
For each pupil compute the time difference between the two running tasks. 
Test whether these time differences are effected by the type of drink.
-->

As is discovered in b), we cannot assume the normality of the population. 
So we use Mann-Whitney test here. 
We set up $H_0$ as there is no difference between the time differences of these two types of drink. 

```{r}
p_val <- wilcox.test(run_energy$before - run_energy$after, run_lemo$before - run_lemo$after)
```

We get $p$ = `r p_val`. 
So we don't reject null hypothesis. 
Therefore, there is no difference between these two populations. 


**d)** 
<!--
Can you think of a plausible objection to the design of the experiment in b) 
if the main aim was to test whether drinking the energy drink speeds up the running? 
Is there a similar objection to the design of the experiment in c)? 
Comment on all your findings in this exercise.
-->

If the main aim is to test whether drinking the energy drink speeds up the running, 
then we needs to get higher sensitivity and power. 
For tests, the power is positively associated with sample size.
For both b) and c), the sample size is too small. 

Under current design of the experiment, we find that the drinks don't have much effect. 

## Exercise 5
<!--
The dataset chickwts is a data frame included in the standard R installation, to view it, type chickwts at the R prompt. 
This data frame contains 71 observations on newly-hatched chicks which were randomly allocated among six groups. 
Each group was given a different feed supplement for six weeks, after which their weight (in grams) was measured. 
The data frame consists of a numeric column giving the weights, and a factor column giving the name of the feed supplement.
-->

**a)** 
<!--
Test whether the distributions of the chicken weights for meatmeal and sunflower groups are different by performing three tests: 
- the two samples t-test (argue whether the data are paired or not)
- the Mann-Whitney test
- the Kolmogorov-Smirnov test. 
Comment on your findings.
-->

The data are not paired, because the experiment units are not the same, 
which leads to the number of the samples in these two groups are different. 
We set up $H_0$` as the distribution between the weight after using meatmeal and sunflower is the same.

Firstly, we perform the not-paired two samples t-test. 

```{r}
meatmeal <- subset(chickwts, feed=="meatmeal")$weight
sunflower <- subset(chickwts, feed=="sunflower")$weight
p_val <- t.test(meatmeal, sunflower)$p.value
```

The result indicates that `p` = `r p_val`. For $\alpha = 0.05$, we reject the null hypothesis, 
that is, the distribution between the weight after using meatmeal and sunflower is not the same.

Secondly, we perform the Mann-Whitney test. 

```{r}
p_val <- wilcox.test(meatmeal, sunflower)$p.value
```

$p$ = `r p_val`. 
For $\alpha = 0.05$, we cannot reject $H_0$`, the populations are the same.

At last, we perform Kolmogorov-Smirnov test. 

```{r}
p_val <- ks.test(meatmeal, sunflower)$p.value
```

$p$ = `r p_val`. For $\alpha = 0.05$, we again cannot reject $H_0$`.

We find that only t-test rejects the null hypothesis. 
After examining the assumption of the tests, we think the t-test cannot be used here. 
We examine the normality using QQ-plots.

```{r, fig.width = 6, fig.height = 3.5}
par(mfrow=c(1, 2))
qqnorm(meatmeal); qqnorm(sunflower)
```

According to the plots, we cannot assume the normality, especially for the distribution of sunflower. 

**b)** 
<!--
Conduct a one-way ANOVA to determine whether the type of feed supplement has an effect on the weight of the chicks. 
Give the estimated chick weights for each of the six feed supplements. 
What is the best feed supplement?
-->

We set up $H_0$` as all feed supplement types have same effects. 
Then we perfoem ANOVA as follows. 

```{r}
chickwts_lm <- lm(weight~feed, data=chickwts)
p_val <- anova(chickwts_lm)["feed", ]$p.value
```

For $\alpha = 0.05$, $p$ = `r p_val`. 
Since $p$ < $\alpha$, we reject $H_0$ and support the claim. 
At least one feed supplement has significant impact.

```{r}
coef <- summary(chickwts_lm)$coefficients[, "Estimate"]
```

The estimation results are listed as follows: 
casein: `r coef["(Intercept)"]`, 
horsebean: `r coef["(Intercept)"] + coef["feedhorsebean"]`, 
linseed: `r coef["(Intercept)"] +  coef["feedlinseed"]`, 
meatmeal: `r coef["(Intercept)"] +  coef["feedmeatmeal"]`, 
soybean: `r coef["(Intercept)"] +  coef["feedsoybean"]`, 
sunflower: `r coef["(Intercept)"] +  coef["feedsunflower"]`. 

So, sunflower is the best feed supplemnts. 

**c)** 
<!--
Check the ANOVA model assumptions by using relevant diagnostic tools.
-->

```{r, fig.width = 6, fig.height = 3.5}
par(mfrow=c(1, 1))
qqnorm(residuals(chickwts_lm))
```

Through checking normality of the residuals using QQ-plot, 
we can confirm that the assumptions of noramlity of ANOVA satisfies. 

**d)** 
<!--
Does the Kruskal-Wallis test arrive at the same conclusion about the effect of feed supplement as the test in b)? 
Explain possible differences between conclusions of the Kruskal-Wallis and ANOVA tests.
-->

```{r}
p_val <- kruskal.test(weight~feed, data=chickwts)$p.value
```

$p$ = `r p_val`. 
For $\alpha = 0.05$, we reject $H_0$, 
which means at least 2 distributions of the weights after feeding different feed supplement are different. 
So ANOVA test and Kruskal-Wallis test arrive at the same conclusion, and there is no significant difference. 
From the QQ-plot in c), we can assume the normality, so both tests can be used here. 
