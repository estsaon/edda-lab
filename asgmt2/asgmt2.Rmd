---
title: "Assignment 2"
author: 'Group 59: Peter Pan, Yuxiang Wang, Yancheng Zhuang'
date: "8 March 2021"
output: pdf_document
fontsize: 11pt
highlight: tango
header-includes: \usepackage{bbm}
---

`r options(digits=3)`

## Exercise 1 
<!--
If left alone bread will become moldy, rot or decay otherwise. 
To investigate the influence of temperature and humidity on this process, 
the time to decay was measured for 18 slices of white bread, 
which were placed in 3 different environments and humidified or not. 
The data are given in the file bread.txt, with 
    the first column time to decay in hours, 
    the second column the environment (cold, warm or intermediate temperature),  
    the third column the humidity.
-->

**a)** 
<!--
The 18 slices came from a single loaf, but were randomized to the 6 combinations of conditions. 
Present an R-code for this randomization process.
-->

```{r}
I<-3; J<-2; N<-3; 
head(t(rbind(
    rep(c("cold", "intermedia", "warm"), each=N*J), 
    rep(c("dry", "wet"), N*I), 
    sample(1:(N*I*J))
)), n=5L)
```

Note that here just shows part of the results due to the lack of space. 

**b)** 
<!--
Make two boxplots of hours versus the two factors and two interaction plots 
(keeping the two factors fixed in turn).
-->

```{r, fig.height=2.5, fig.width=6, fig.align='center'}
data_bread <- read.table("bread.txt", header=TRUE)
par(mfrow=c(1, 2), mar=c(5, 2.5, 0, 2.5))
boxplot(hours~environment, data=data_bread); boxplot(hours~humidity, data=data_bread)
```

```{r, fig.height=3, fig.width=6, fig.align='center'}
with(data_bread, {
        par(mfrow=c(2, 1), mar=c(3, 0, 0, 0))
        interaction.plot(environment, humidity, hours)
        interaction.plot(humidity, environment, hours)
    }
)
```

The lines in the interaction plot are unparallel, there might exists interactions between temperature and humidity. 

**c)** 
<!--
Perform an analysis of variance to test for effect of the factors temperature, humidity, and their interaction. 
Describe the interaction effect in words.
-->

We use 2-way ANOVA test to perform the test.
We formalize the hypothesis to test as follows: 
1) $H_A$ as $\alpha_i = 0$ for all $i$; 
2) $H_B$ as $\beta_j = 0$ for all $j$; 
3) $H_{AB}$ as $\gamma_{ij} = 0$ for all ($i$,$j$).

```{r}
bread_lm <- lm(hours ~ environment * humidity, data=data_bread)
res <- anova(bread_lm)
subset(janitor::row_to_names(rbind(rownames(res), res$Pr), 1), select=-c(Residuals))
```

Then we investigate the significant of individual factors and their interaction through summary.

```{r}
coef <- summary(bread_lm)$coefficients; coef
```

According to the analysis of variance and summary,
we can notice that: 
1) $p$ of factor temperature is `r res$Pr[1]`, 
$p$ of factor humidity is `r res$Pr[2]`, 
and $p$ of two factors interaction is `r res$Pr[3]`. 
Since $p$-values are all less than $\alpha = 0.05$, so we reject $H_A$, $H_B$ and $H_AB$
that is to say both of the factors and their interaction have great effect on the decay.
2) For the interaction of intermediate environment and wet humidity, 
$p$-value = `r coef["environmentintermediate:humiditywet", "Pr(>|t|)"]` < 0.05, 
which indicates there is a significant interaction between intermediate temperature and wet humidity. 
The case is the same for interaction of warm environment and wet humidity, 
$p$-value = `r coef["environmentwarm:humiditywet", "Pr(>|t|)"]` < 0.05. 

**d)** 
<!--
Which of the two factors has the greatest (numerical) influence on the decay? 
Is this a good question?
-->

As we see, the previous analysis concludes that there are significant interaction between the two factors. 
We use addictive model to summary, which eliminates their interaction effects. 

```{r}
bread_addctive_lm <- lm(hours ~ environment + humidity, data = data_bread) 
summary(bread_addctive_lm)$coefficients
```

We find that the factors of wet humidity and warm temperature have the largest estimate values, 
so they have the greatest influence on the decay.
Nevertheless, we think it is not a good question due to the existence of interaction, 
since we cannot ignore the effect of interaction between the two factors on the decay.

**e)** 
<!--
Check the model assumptions by using relevant diagnostic tools. 
Are there any outliers?
-->

We check the normality and the assumption of equal variances. 
The left plot is the distribution of residuals, which should look normal. 
The right plot is the fitted plot versus residuals, there should not exist systematically pattern. 

```{r, fig.height=2, fig.width=6, fig.align='center'}
par(mfrow=c(1, 2), mar=c(3, 5, 1, 0))
qqnorm(residuals(bread_lm), main=NULL); plot(fitted(bread_lm), residuals(bread_lm))
```

Left plot: normality is doubtful, the points are not approximately on a straight line. 
Right plot: the spread of residuals seems to be symmetric with fitted values, 
and some data points seem relatively bigger, so there could exist outliers.

## Exercise 2 
<!--
A researcher is interested in the time it takes a student to find a certain product on the internet using a search engine. 
There are three different types of interfaces with the search engine and especially the effect of these interfaces is of importance. 
There are five different types of students, indicating their level of computer skill
(the lower the value of this indicator, the better the computer skill of the corresponding student). 
Fifteen students are selected; three from each group with a certain level of computer skill. 
The data is given in the file search.txt. 
Assume that the experiment was run according to a randomized block design which you make in a). 
(Beware that the levels of the factors are coded by numbers.)
-->

**a)** 
<!--
Number the selected students 1 to 15 and 
show how (by using R) the students could be randomized to the interfaces in a randomized block design.
-->

We take the interface as treatment and regard the skill as block factor, each combination only tests 1 time. 
Then we randomize students.

```{r}
I <- 3; B <- 5; N <- 1
for(i in 1:B) print(sample(1:(N * I)))
```

Each row is a block level, 3 treatments are randomly distributed in each block.

**b)** 
<!--
Test the null hypothesis that the search time is the same for all interfaces. 
What type of interface does require the longest search time? 
For which combination of skill level and type of interface is the search time the shortest? 
Estimate the time it takes a typical user of skill level 3 to find the product on the website if the website uses interface 3.
-->

We set hypothesis $H_0$ as the interfaces have no impact in the search time. 
We use both boxplot and interaction plot to have a graphical summary. 

```{r, fig.height=2, fig.width=6, fig.align='center'}
data_search <- read.table("search.txt", header=TRUE)
data_search$skill <- as.factor(data_search$skill)
data_search$interface <- as.factor(data_search$interface)
par(mfrow=c(1, 2), mar=c(3, 5, 1, 0))
boxplot(time~skill, data=data_search); boxplot(time~interface, data=data_search)
```

Left plot: we can notice that different skills can lead to different time. 
The higher the skill factor leads to the longer time to complete the task. 
Right plot: we can again notice that different interfaces can also lead to different time. 

```{r, fig.height=3, fig.width=6, fig.align='center'}
with(data_search, {
    par(mfrow=c(2, 1), mar=c(3, 0, 0, 0))
    interaction.plot(skill, interface, time)
    interaction.plot(interface, skill, time)
})
```

The lines in the interaction plot are unparallel, 
there might exists interactions between the skill and the interface used to complete the task.

We set up $H_0$ as the search time is the same for all 3 interfaces.
Then we use ANOVA test under the assumption that there is no interaction between block and treatment.

```{r}
contrasts(data_search$interface) <- contr.sum
contrasts(data_search$skill) <- contr.sum
search_lm <- lm(time ~ interface + skill, data=data_search)
res <- anova(search_lm); p_value <- res$Pr; res
```

The $p$ of interface is `r p_value[1]`, which is smaller than 0.05. 
Hence we reject $H_0$ and conclude that the interfaces used affect the search time.

```{r}
coef <- summary(search_lm)$coefficients[, "Estimate"]; coef
```

The estimation results are listed as follows: 
interface 1: `r coef["(Intercept)"] + coef["interface1"]`, 
interface 2: `r coef["(Intercept)"] + coef["interface2"]`, 
interface 3: `r coef["(Intercept)"] + (0 - sum(coef[2:3]))`,
skill 1: `r coef["(Intercept)"] + coef["skill1"]`, 
skill 2: `r coef["(Intercept)"] + coef["skill2"]`, 
skill 3: `r coef["(Intercept)"] + coef["skill3"]`,
skill 4: `r coef["(Intercept)"] + coef["skill4"]`, 
skill 5: `r coef["(Intercept)"] + (0 - sum(coef[4:6]))`.

So, interface 3 require the longest search time. 
The combination of skill level 1 and interface 1 requires the shortest search time.
We get the estimation time of the combination of skill level 3 and interface 3 is 
$\mu + \alpha_3 + \beta_3 =$  `r coef["(Intercept)"] + (0 - sum(coef[2:3])) + coef["skill3"]`.

**c)** 
<!--
Check the model assumptions by using relevant diagnostic tools.
-->

We check the normality and assumptions by QQ-plot and interaction plot.

```{r, fig.height=2, fig.width=6, fig.align='center'}
par(mfrow=c(1, 2), mar=c(3, 5, 1, 0))
qqnorm(residuals(search_lm)); plot(fitted(search_lm), residuals(search_lm))
```

Left plot: data points are approximately on the straight line. 
Right plot: there is no systematic change in the residuals against on the fitted values. 
Thus we can assume normality.

**d)** 
<!--
Perform the Friedman test to test whether there is an effect of interface.
-->

We use the Friedman test to test whether there is an effect of interface.

```{r}
p_value <- with(data_search, friedman.test(time, interface, skill)$p.value)
```

Since the $p$ = `r p_value` < $\alpha = 0.05$, we reject $H_0$, there is a treatment effect. 

**e)** 
<!--
Test the null hypothesis that the search time is the same for all interfaces by a one-way ANOVA test, ignoring the variable skill. 
Is it right/wrong or useful/not useful to perform this test on this dataset?
-->

We perform the one-way anova test and ignore the variable skill.

```{r}
data_search <- read.table("search.txt", header=TRUE)
search_oneway_lm <- lm(time~interface, data=data_search)
res <- anova(search_oneway_lm); p_value <- res$Pr[1]; res
```

We get the $p$ = `r p_value` < $\alpha = 0.05$. 
Hence, we reject $H_0$, the factor interface is significant. 
However, it's wrong to perform this test on this dataset, because these samples don't have equal variances. 
Moreover, there could exist interaction between two factors.

## Exercise 3
<!--
In a study on the effect of feedingstuffs on lactation, 
a sample of nine cows were fed with two types of food, and their milk production was measured. 
All cows were fed both types of food, during two periods, 
with a neutral period in-between to try and wash out carry-over effects. 
The order of the types of food was randomized over the cows. 
The observed data can be found in the file cow.txt, where A and B refer to the types of feedingstuffs.
-->

**a)** 
<!--
Test whether the type of feedingstuffs influences milk production using an ordinary “fixed effects” model, fitted with lm. 
Estimate the difference in milk production.
-->

We set up $H_0$ as 
1) cow factor does not affect lactation; 
2) the first treatment or the second treatment does not affect lactation;
3) the type of feedingstuffs/treatment does not affect lactation. 

```{r}
cow <- read.table("cow.txt", sep="")
cow$id <- factor(cow$id); cow$per <- factor(cow$per)
cow_lm <- lm(milk ~ id + per + treatment, data=cow)
result <- anova(cow_lm); result
```

The results show that cow factor (p = `r result["id", ]$Pr` < 0.05) and 
whether a cow goes through its first treatment or second (p = `r result["per", ]$Pr` < 0.05) do affect lactation. 
But the factor of interest, namely type of treatment (p = `r result["treatment", ]$Pr` > 0.05) does not affect lactation significantly. 

```{r}
result <- summary(cow_lm)$coefficients; result
```

We can see that the difference in milk production is estimated as `r result["treatmentB", "Estimate"]`, 
so treatment B has a lower lactation compared to treatment A. 

**b)** 
<!--
Repeat a) and b) by performing a mixed effects analysis, modelling the cow effect as a random effect (use the function lmer). 
Compare your results to the results found by using a fixed effects model. 
(You will need to install the R-package lme4, which is not included in the standard distribution of R.)
-->

```{r}
cow_lmer <- lme4::lmer(milk ~ treatment + order + per + (1 | id), data=cow, REML=FALSE)
cow_lmer_without_treatment <- lme4::lmer(milk ~ order + per + (1 | id), data=cow, REML=FALSE)
result <- anova(cow_lmer_without_treatment, cow_lmer)
```

According to the result (p = `r result["cow_lmer", "Pr(>Chisq)"]` > 0.05), 
modelling the cow effect as a random effect still conclude that the type of the treatment
does not significantly affect the lactation, which is the same with the previous fixed effect model. 

```{r}
cow_lmer_without_per <- lme4::lmer(milk ~ treatment + order + (1 | id), data=cow, REML=FALSE)
result <- anova(cow_lmer_without_per, cow_lmer)
```

Whether a cow goes through its first treatment or second (p = `r result["cow_lmer", "Pr(>Chisq)"]` < 0.05) 
still affect lactation significantly. 

**c)** 
<!--
Study the commands:
> attach(cow)
> t.test(milk[treatment=="A"], milk[treatment=="B"], paired=TRUE)
Does this produce a valid test for a difference in milk production? 
Is its conclusion compatible with the one obtained in a)? 
Why?
-->

```{r}
result <- with(cow, t.test(milk[treatment=="A"], milk[treatment=="B"], paired=TRUE)); result
```

According to the result, the p-value of the t-test is `r result$p.value` which is greater than 0.05, results in not rejecting the null hypothesis. 
But t-test might be not suitable here, because: 
1) only the impact of treatment on milk production is tested, and the impact of other factors is not considered; 
2) repeated measures may not be exchangeable because of 
time effect (cows are growing) and learning effect (cows become used to the feedingstuffs).

But the conclusion is still compatible with the one obtained in a), or even identical p-value if we modeling ignoring per factor as follows: 
```{r}
anova(lm(milk ~ treatment + id, data=cow))
```

This is because F-test is the generalization of t-test.  
In the case of two levels of repeated measurements, 
F-statistic of the ANOVA is identical to the t-statistic for paired t-test. 

## Exercise 4
<!--
Stochastic models for word counts are used in quantitative studies on literary styles. 
Statistical analysis of the counts can, for example, be used to solve controversies about true authorships. 
Another example is the analysis of word frequencies in relation to Jane Austen’s novel Sanditon. 
At the time Austen died, this novel was only partly completed. 
Austen, however, had made a summary for the remaining part. 
An admirer of Austen’s work finished the novel, imitating Austen’s style as much as possible. 
The file austen.txt contains counts of different words in some of Austen’s novels: 
    chapters 1 and 3 of Sense and Sensibility (stored in the Sense column), 
    chapters 1, 2 and 3 of Emma (column Emma), 
    chapters 1 and 6 of Sanditon (both written by Austen herself, column Sand1) and 
    chapters 12 and 24 of Sanditon (both written by the admirer, Sand2).
-->

**a)** 
<!--
Discuss whether a contingency table test for independence or for homogeneity is most appropriate here.
-->

In order to analyze the word frequencies in relation to Jane Austen’s novel Sanditon, 
we need to check whether the distributions over column factors/word counts are equal between the original work of Austen and her admirers. 
So, a contingency table test for homogeneity is most appropriate here.

**b)** 
<!--
Using the given data set, investigate whether Austen herself was consistent in her different novels.
Where are the main inconsistencies?
-->

Null hypothesis: Austin herself is consistent in her different novels
Alternative hypothesis: Austin herself is inconsistent in her different novels

```{r}
austen <- read.table("austen.txt", sep="")
z <- chisq.test(austen[, c('Sense', 'Emma', 'Sand1')]); z
```
According to the above results, the p value is `r z$p.value`, which is greater than 0.05. 
Therefore, we don't reject the original hypothesis and consider that the novels are consistent with each other.

```{r}
z$residuals
```

We look at the contributions of each cell to the chi-squared statistics to determine the main inconsistencies. 
We find: 
1) chapters 1 and 6 of Sanditon have relatively more 'a'; 
2) chapters 1, 2 and 3 of Emma have relatively fewer 'a', 'an', 'that' and 'without';  
3) chapters 1 and 3 of Sense and Sensibility have have relatively fewer 'a' and 'with'.

**c)** 
<!--
Was the admirer successful in imitating Austen’s style? 
Perform a test including all data. 
If he was not successful, where are the differences?
-->

```{r}
z <- chisq.test(austen); z
```

As can be seen from the result, 
the p-value is `r z$p.value` < 0.05 after adding the admirer's word frequencies, 
which lead us to reject the null hypothesis. 
Hence, the admirer wasn’t successful in imitating Austen’s style.

```{r}
z$residuals
```

We check the residuals again and find that the admirer uses relatively more "an", "with" and fewer "that". 

## Exercise 5
<!--
The data in expensescrime.txt were obtained to 
determine factors related to state expenditures on criminal activities (courts, police, etc.) 
The variables are: 
    state (indicating the state in the USA), 
    expend (state expenditures on criminal activities in $1000), 
    bad (crime rate per 100000), 
    crime (number of persons under criminal supervision), 
    lawyers (number of lawyers in the state), 
    employ (number of persons employed in the state),  
    pop (population of the state in 1000). 
In the regression analysis, take 
    expend as response variable, 
    bad, crime, lawyers, employ and pop as explanatory variables.
-->

**a)** 
<!--
Make some graphical summaries of the data. 
Investigate the problem of potential and influence points, and the problem of collinearity.
-->
```{r, fig.height=3, fig.width=6, fig.align='center'}
expenses_crime <- read.table("expensescrime.txt", header=TRUE)
cols <- c("expend", "bad", "crime", "lawyers", "employ", "pop")
par(mfrow=c(2, 3))
for (col in cols) hist(expenses_crime[col][, 1], main=col, xlab=NULL)
```

According to the histogram plot, 
we can see bad, lawyers, employ and pop variables have influence points. 
We use Cook's distance here to find these influence points. 

```{r, fig.height=1.5, fig.width=6, fig.align='center'}
expenses_crime_lm <- lm(expend ~ bad + crime + lawyers + employ + pop, data=expenses_crime)
ckd <- olsrr::ols_prep_cdplot_data(expenses_crime_lm, type=3)$ckd
expenses_crime[ckd[ckd$color=="outlier", c("obs")], c("state")]
olsrr::ols_plot_cooksd_chart(expenses_crime_lm, type=3)
```

We set up the threshold as 1, which leads to 4 influence points: 
observation 5, 8, 35 and 44 (CA, FC, NY and TX respectively). 

We can use the scatter plot shown above to check pairwise collinearities. 
Correlation test is also used here. 
Because according to the histogram, we can not assume normality here, 
so we choose Spearman's rank correlation test. 

```{r}
cor(expenses_crime[cols], method="spearman")
```

We can see pairwise collinearities between 
bad and lawyers, bad and employ, bad and pop, 
lawyers and employ, lawyers and pop, employ and pop. 

We also use variance inflation factor to check the collinearities. 

```{r}
car::vif(expenses_crime_lm)
```

According to the VIF, we are sure that there is a collinearity problem. 

**b)** 
<!--
Fit a linear regression model to the data. 
Use both the step-up and the step-down method to find the best model. 
If step-up and step-down yield two different models, choose one and motivate your choice. 
-->
Firstly, we use step-up method. 

```{r}
step_up <- function(data, response, terms, variables) {
    r.squared <- c()
    for (variable in variables) {
        if (is.null(terms)) {
            this_r.squared <- summary(lm(paste(response, "~", variable), data=data))$r.squared
        } else {
            terms <- paste(terms, collapse='+')
            this_r.squared <- summary(
                lm(paste(response, "~", terms, "+", variable), data=data))$r.squared
        }; r.squared <- c(r.squared, this_r.squared)
    }; print(t(data.frame(variables, r.squared)))
}
variables <- cols[! cols %in% c("expend")]
step_up(expenses_crime, "expend", NULL, variables)
```

The employ variable has the highest $R^2$, so we select employ variable.

```{r}
variables <- variables[! variables %in% c("employ")]
step_up(expenses_crime, "expend", c("employ"), variables)
```

The lawyers variable yields the highest $R^2$ increase, so we add it the model. 

```{r}
variables <- variables[! variables %in% c("lawyers")]
step_up(expenses_crime, "expend", c("employ", "lawyers"), variables)
```

Add bad, crime or pop variable does not yield significant increase, therefore we stops. 

Then, we use step-down strategy to choose the variable. 

```{r}
summary(lm(expend~bad+crime+lawyers+employ+pop, data=expenses_crime))$coefficients[, "Pr(>|t|)"]
```

Variable crime has the highest p-value, so we remove it firstly. 

```{r}
summary(lm(expend ~ bad+lawyers+employ+pop, data=expenses_crime))$coefficients[, "Pr(>|t|)"]
```

Similarly, at the level of p=0.05, 
we remove the pop variable which is larger than 0.05 and greatest among the variables. 

```{r}
summary(lm(expend ~ bad+lawyers+employ, data=expenses_crime))$coefficients[, "Pr(>|t|)"]
```

For the same reason, we remove bad here. 

```{r}
coef <- summary(lm(expend ~ lawyers+employ, data=expenses_crime))$coefficients; coef[, "Pr(>|t|)"]
```

All remaining explanatory variables in the model are significant, so we stop here. 
We can see that we get the same result with the step-up method. 
The resulting model is 
expend = `r coef["(Intercept)", "Estimate"]` + `r coef["lawyers", "Estimate"]` * lawyers + `r coef["employ", "Estimate"]` * employ + error

**c)** 
<!--
Check the model assumptions by using relevant diagnostic tools. 
-->
For model assumptions, we need to check both the linearity of the relation and the normality of the errors. 
Firstly, we plot residuals against each `X_k` in the model separately. 
```{r, fig.height=2, fig.width=6, fig.align='center'}
expenses_crime <- expenses_crime[c("expend", "lawyers", "employ")]
lawyers_lm <- lm(expend ~ lawyers, data=expenses_crime)
employ_lm <- lm(expend ~ employ, data=expenses_crime)
par(mfrow=c(1, 2), mar=c(3, 5, 1, 0))
with(expenses_crime, {plot(residuals(lawyers_lm), lawyers); plot(residuals(employ_lm), employ)}) 
```

There is no curved pattern in the scatter plot. 

Secondly, added variable plot / partial regression plot. 

```{r, fig.height=3, fig.width=6, fig.align='center'}
expenses_crime_lm <- lm(expend ~ lawyers + employ, data=expenses_crime)
car::avPlots(expenses_crime_lm)
```

From the plot we can identify influence points like observation 5 and 8.
15 and 32 is dubious, and the results are different from a). 

```{r, fig.height=1.5, fig.width=6, fig.align='center'}
olsrr::ols_plot_cooksd_chart(expenses_crime_lm, type=3)
```

According to Cook's distance again, 5 and 8 can be seen as influence points. 

```{r, fig.height=3, fig.width=6, fig.align='center'}
expenses_crime_lm_filtered <- lm(expend ~ lawyers + employ, data=expenses_crime[-c(5, 8), ])
car::avPlots(expenses_crime_lm_filtered)
```

If we remove these two influence points, 
we can yield better model that 
$R^2$=`r summary(expenses_crime_lm_filtered)$r.squared` > `r summary(expenses_crime_lm)$r.squared`.
Also the linearity of the relation is easier to be seen. 

Lastly, we check the normality using QQ-plot. 

```{r, fig.height=2, fig.width=6, fig.align='center'}
par(mfrow=c(1, 2), mar=c(3, 5, 1, 0))
qqnorm(residuals(expenses_crime_lm), xlab="expenses_crime_lm")
qqnorm(residuals(expenses_crime_lm_filtered), xlab="expenses_crime_lm_filtered")
```

The left one is the previous model, and the right one is the model with influence points filtered as above. 
We can see that the normality of the previous model is dubious, 
but after removing the influence points, the lines are approximately on a single straight line.  

```{r, fig.height=2, fig.width=6, fig.align='center'}
par(mfrow=c(1, 2), mar=c(3, 5, 1, 0))
plot(fitted(expenses_crime_lm), residuals(expenses_crime_lm))
plot(fitted(expenses_crime_lm_filtered), residuals(expenses_crime_lm_filtered))
```
We can also see that there is no systematic change in the residuals against on the fitted values, 
except one point that is 35 might also be considered as an influence point. 
Thus, the normality can be assumed. 
